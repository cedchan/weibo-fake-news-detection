{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzBWsaIr_qJF"
      },
      "source": [
        "Detect whether Weibo microblog post (Chinese) contains fake news"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installations & Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tensorflow\n",
        "%pip install spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load spaCy packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spacy download zh_core_web_lg\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports and Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GSKNqKYomDD",
        "outputId": "fa8ccceb-1620-453e-a4cb-3db27d8f4cd1"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import timeit\n",
        "import spacy\n",
        "import os\n",
        "import re\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from spacy import displacy\n",
        "from spacy.language import Doc\n",
        "\n",
        "FAKE = 1\n",
        "REAL = 0\n",
        "nlp = spacy.load('zh_core_web_lg')\n",
        "incl_punc = False # Keep all punctuation in data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-Process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Clean Dataset Functions TODO：CONSIDER CONVERTING ALL TO SIMPLIFIED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "spaCy Pad Tokens with Spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_tokens(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    padded = Doc(nlp.vocab, words=tokens, spaces=([True] * len(tokens)))\n",
        "    return text # padded.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rumors Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_rumor(data_list):\n",
        "  real_ex = 0\n",
        "  fake_ex = 0\n",
        "  raw_text = ''\n",
        "\n",
        "  with open('data/rumor_data/labels.txt') as f:\n",
        "    labels_str = f.read()\n",
        "\n",
        "  labels_search = re.findall('eid:(\\d+)\\s+label:([01])', labels_str)\n",
        "  labels = dict(labels_search) # populate dictionary with post id:label pairs\n",
        "\n",
        "  for file in os.scandir('data/rumor_data/posts/'):\n",
        "    with open(file, encoding='utf-8') as f:\n",
        "      json_raw = json.load(f)\n",
        "\n",
        "    text = json_raw[0]['text']\n",
        "    clean = re.sub('http:\\/\\/.*( |$)', '', text)\n",
        "    padded = pad_tokens(clean)\n",
        "\n",
        "    id = json_raw[0]['id']\n",
        "    label = int(labels.get(str(id)))\n",
        "    \n",
        "    if (label == REAL):\n",
        "      real_ex += 1\n",
        "    else:\n",
        "      fake_ex += 1\n",
        "    raw_text += clean + ' '\n",
        "    data_list.append([padded, label])\n",
        "\n",
        "  return (real_ex, fake_ex, raw_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CHECKED Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_checked(data_list):\n",
        "    real_ex = 0\n",
        "    fake_ex = 0\n",
        "    raw_text = ''\n",
        "\n",
        "    # Fake news\n",
        "    for file in os.scandir('data/CHECKED_data/fake_news/'):\n",
        "        with open(file, encoding='utf-8') as f:\n",
        "            json_raw = json.load(f)\n",
        "        \n",
        "        text = json_raw['text']\n",
        "        clean = re.sub('(\\\\u200b|\\\\xa0|\\\\ue41d|\\\\ue627|\\\\u3000|\\\\u2005)', '', text)\n",
        "        padded = pad_tokens(clean)\n",
        "\n",
        "        fake_ex += 1\n",
        "        raw_text += clean + ' '\n",
        "        data_list.append([padded, FAKE])\n",
        "\n",
        "    # Real news\n",
        "    for file in os.scandir('data/CHECKED_data/real_news/'):\n",
        "        with open(file, encoding='utf-8') as f:\n",
        "            json_raw = json.load(f)\n",
        "        \n",
        "        text = json_raw['text']\n",
        "        clean = re.sub('(\\\\u200b|\\\\xa0|\\\\ue41d|\\\\ue627|\\\\u3000|\\\\u2005)', '', text)\n",
        "        padded = pad_tokens(clean)\n",
        "        \n",
        "        real_ex += 1\n",
        "        raw_text += clean + ' '\n",
        "        data_list.append([padded, REAL])\n",
        "\n",
        "    return(real_ex, fake_ex, raw_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "COVID-19 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_covid(data_list):\n",
        "  real_ex = 0\n",
        "  fake_ex = 0\n",
        "  raw_text = ''\n",
        "\n",
        "  with open('data/COVID19_data/weibo_covid19_label.txt') as f:\n",
        "    labels_str = f.read()\n",
        "\n",
        "  with open('data/COVID19_data/weibo_covid19_data.txt', encoding='utf-8') as f:\n",
        "    text_str = f.read()\n",
        "  text_search = re.findall('(\\d+)\\s+None\\s+\\d+\\s+\\d+\\.*\\d*\\s+(.+)', text_str)\n",
        "  text = dict(text_search) # populate dictionary with post id:text pairs\n",
        "\n",
        "  for (id, text) in text_search:\n",
        "    labels = re.findall(f'{id}\\s+([01])', labels_str)\n",
        "    if (len(labels) == 0): continue\n",
        "    label = labels[0]\n",
        "    padded = pad_tokens(text)\n",
        "\n",
        "    if (label == REAL):\n",
        "      real_ex += 1\n",
        "    else:\n",
        "      fake_ex += 1\n",
        "    raw_text += text + ' '\n",
        "    data_list.append([padded, label])\n",
        "  \n",
        "  return (real_ex, fake_ex, raw_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clean and Save Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_and_save():\n",
        "    data_list = []\n",
        "    \n",
        "    (real_rumor, fake_rumor, raw_rumor) = clean_rumor(data_list)\n",
        "    (real_checked, fake_checked, raw_checked) = clean_checked(data_list)\n",
        "    (real_covid, fake_covid, raw_covid) = clean_covid(data_list)\n",
        "    \n",
        "    real_ex = real_rumor + real_checked + real_covid\n",
        "    fake_ex = fake_rumor + fake_checked + fake_covid\n",
        "    raw_text = raw_rumor + raw_checked + raw_covid\n",
        "\n",
        "    data = pd.DataFrame(data_list, columns=['Text', 'Label'])\n",
        "    data.to_csv('data/cleaned_data.csv', index=False)\n",
        "    \n",
        "    testj = {\n",
        "        'real_ex': real_ex,\n",
        "        'fake_ex': fake_ex,\n",
        "        'raw_text': raw_text\n",
        "    }\n",
        "    with open('data/metadata.json', mode='w') as f:\n",
        "      json.dump(testj, f, indent=4)\n",
        "\n",
        "    print(f'Real Examples: {real_ex}\\nFake Examples: {fake_ex}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if ((not os.path.exists('data/cleaned_data.csv')) or (not os.path.exists('data/metadata.json'))):\n",
        "  clean_and_save()\n",
        "\n",
        "dataset = pd.read_csv('data/cleaned_data.csv').dropna()\n",
        "texts = dataset['Text'].to_numpy()\n",
        "labels = dataset['Label'].to_numpy()\n",
        "\n",
        "with open('data/metadata.json', encoding='utf-8', mode='r') as f:\n",
        "  metadata = json.load(f)\n",
        "\n",
        "real_ex = metadata['real_ex']\n",
        "fake_ex = metadata['fake_ex']\n",
        "raw_text = metadata['raw_text']\n",
        "\n",
        "print(f''' \n",
        "  •=============================•\n",
        "  |         DATA SUMMARY        |\n",
        "  •=============================•\n",
        "  | Real Examples: {real_ex}         |\n",
        "  | Fake Examples: {fake_ex}         |\n",
        "  | Total: {real_ex + fake_ex}                 |\n",
        "  |-----------------------------|\n",
        "  | Actual Size: {len(dataset)}           |\n",
        "  •=============================•\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# VECTORIZING\n",
        "\n",
        "Vectorize_layer = layers.TextVectorization()\n",
        "Vectorize_layer.adapt(texts)\n",
        "vocab = Vectorize_layer.get_vocabulary()\n",
        "num_tokens = len(vocab)\n",
        "print(\"num\", num_tokens)\n",
        "\n",
        "# EMBEDDING MATRIX\n",
        "\n",
        "embedding_dim = len(nlp('曾').vector) # Finds length of arbitrary spaCy word vector \n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "hits, misses = 0, 0\n",
        "\n",
        "for i, word in enumerate(vocab):\n",
        "  token = nlp(word)\n",
        "  if token.has_vector:\n",
        "    hits += 1\n",
        "    embedding_matrix[i] = token.vector\n",
        "  else:\n",
        "    misses += 1\n",
        "print(f'hits: {hits}, misses: {misses}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Embedding_layer = layers.Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=True,\n",
        ")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    Vectorize_layer,\n",
        "    Embedding_layer,\n",
        "    layers.LSTM(2, return_sequences=True), # dimension size?\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.GlobalAveragePooling1D(), # what does this do\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
        ")\n",
        "\n",
        "# print(model.summary())\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    texts,\n",
        "    labels, # CHANGE TO NUMBERS\n",
        "    batch_size = 50,\n",
        "    epochs = 10,\n",
        "    validation_split = 0.2 # fix to split data later\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit ('anaconda3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "722344e03542fabc1ef0b63553515a0d1d2a2145a1410551ac5799b74536ab30"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
